{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5ca3109",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# HSTaXe Cookbook: Spectral Extraction for WFC3/IR \n",
    "\n",
    "This notebook contains a step-by-step guide for performing spectral extractions with HSTaXe for G102 (or G141) data from WFC3/IR. <br>\n",
    "The original source for this notebook is the \"cookbook\" folder on the [spacetelescope/hstaxe](https://github.com/spacetelescope/hstaxe) GitHub repository. \n",
    "\n",
    "***\n",
    "## Learning Goals\n",
    "In this tutorial, you will:\n",
    "\n",
    "- Organize input data\n",
    "- Run custom background subtraction code\n",
    "- Set up HSTaXe and prepare data for extraction\n",
    "- Learn how to handle different types of background subtraction\n",
    "- Extract 1-D spectra with a simple box extraction\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "[1. Introduction](#intro)<br>\n",
    "[2. Imports](#import)<br>\n",
    "[3. Setup](#setup)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.1 Run WFC3 Backsub and Calibrate](#cal)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.2 Verify Matching WCS Information](#wcs)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.3 Drizzling Input Data](#drizzle)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.4 Creating a Catalog with SExtractor](#catalog)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[3.5 Copy Catalog and Rename Mag Column](#copycat)<br>\n",
    "[4. Running HSTaXe](#axe)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[4.1. Outputs](#out) <br>\n",
    "[5. Fluxcube Extraction](#fluxcube)<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[5.1 Building the Fluxcube](#fcubeprep) <br>\n",
    "[6. Conclusions](#conclusions)<br>\n",
    "[7. About this Notebook](#about)<br>\n",
    "[8. Citations](#cite)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2407f8e9",
   "metadata": {},
   "source": [
    "# 1. Introduction <a id=\"intro\"></a>\n",
    "\n",
    "[HSTaXe](https://hstaxe.readthedocs.io/en/latest/index.html) is a Python package that provides spectral extraction processes for HST data. **Please be aware that running this notebook requires creating a conda environment from the provided `.yaml` file in this notebook's [github repository](https://github.com/spacetelescope/hstaxe/tree/main/cookbooks).** For more details about creating the necessary environment see the notebook's README file.\n",
    "\n",
    "Below, we show two workflows for spectral extraction using WFC3/IR G102 grism data. The first workflow performs a basic image-by-image box extraction, while the latter uses a flux cube technique. **The example data we use in this notebook (from WFC3 CAL program 16582) are available [here](https://stsci.box.com/s/j2ygj4gaqgzmp0b4xcc1h2rszz6cv9wm).** If you would like to use this notebook with the example data, please download the `example_data` subdirectory from the link above, and store it within the same parent directory as this notebook. Once you have the example data directory, this notebook is intended to run continuously without needing to edit any of the cells. \n",
    "\n",
    "In addition to the example data, **this notebook also requires configuration files for HSTaXe, which can be downloaded [here](https://stsci.app.box.com/folder/191816748622).** The `WFC3_IR_conf` directory should be stored in the  same parent directory as this notebook, and later we will be copy them to the `CONF` subdirectory created by HSTaXe.\n",
    "\n",
    "**When analyzing WFC3/IR grism data, it is strongly advised that you calibrate the raw grism FITS files with the program WFC3 Backsub available [here](https://stsci.app.box.com/folder/198794823506).** In order to run this notebook, please download the entire `WFC3_Backsub` folder from the provided link, which includes `back_sub.py` as well as a folder of reference FITS files called `backsub_data`, and put it in the same parent directory as this notebook.\n",
    "\n",
    "**If you plan to use your own data with this notebook, please be aware you will be required to create an input source catalog with SExtractor.** Information regarding SExtractor including installation instructions are available [here](https://sextractor.readthedocs.io/en/latest/Installing.html). In addition to installing SExtractor, you must run the software with aXe specific configuration files. **These aXe-SExtractor configuration files can be downloaded [here](https://stsci.box.com/s/3npry36gu7ocfnuxgzwr5syj4i8r7hy8).** Once SExtractor is installed, create a `sextractor` directory in the same parent directory as this notebook, and place configuration files inside. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a91e02da",
   "metadata": {},
   "source": [
    "# 2. Imports <a id=\"import\"></a>\n",
    "\n",
    "For this workflow, we will import the following modules:\n",
    "\n",
    "- *os*, *glob* and *shutil*, for file handling\n",
    "- *numpy* for array handling\n",
    "- *astropy.io.fits* for FITS file handling\n",
    "- *astropy.table.Table* for table functions\n",
    "- *ginga.util.zscale* for image display scaling\n",
    "- *matplotlib.pyplot* for plotting\n",
    "- *stwcs.updatewcs* for matching grism and direct image WCS\n",
    "- *astrodrizzle* for creating input image mosaics\n",
    "- *hstaxe.axetasks* for performing the spectral extraction\n",
    "- *wfc3tools.calwf3* for background subtraction and image reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e99ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "from astropy.io import fits, ascii\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "from ginga.util import zscale\n",
    "from stwcs import updatewcs\n",
    "from drizzlepac import astrodrizzle\n",
    "from hstaxe import axetasks\n",
    "from wfc3tools import calwf3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3915d574",
   "metadata": {},
   "source": [
    "## 3. Setup <a id=\"setup\"></a>\n",
    "\n",
    "We'll start our basic extraction workflow by organizing our input data. A set of example data are available [here](https://stsci.box.com/s/tpbhvrqtbtwod7tr7uijexttoocy4duj) for tutorial purposes. Note that many of the following steps will pre-process data that will only be used for the advanced extraction later in the notebook.\n",
    "\n",
    "First, we save the working directory for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c7dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(f'The current directory is: {cwd}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b237e",
   "metadata": {},
   "source": [
    "Next, we'll create directories for our grism and direct images. **HSTaXe will modify our input images in-place, so it is crucial to retain clean versions of them in another location, which will be copied into these directories.** If running this notebook multiple times, run all the lines in the next cell to clear any existing inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b991be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[39m.\u001b[39mchdir(cwd)\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(\u001b[39m'\u001b[39m\u001b[39mg102\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m      3\u001b[0m     shutil\u001b[39m.\u001b[39mrmtree(\u001b[39m'\u001b[39m\u001b[39mg102\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.chdir(cwd)\n",
    "if os.path.isdir('g102'):\n",
    "    shutil.rmtree('g102')\n",
    "if os.path.isdir('f098m'):\n",
    "    shutil.rmtree('f098m')\n",
    "if os.path.isdir('f105w'):\n",
    "    shutil.rmtree('f105w')\n",
    "os.mkdir('g102')\n",
    "os.mkdir('f098m')\n",
    "os.mkdir('f105w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9629fddd",
   "metadata": {},
   "source": [
    "Now, copy your images to the input directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47783cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# src = '/path/to/your/grism/images/*raw.fits'\n",
    "src = 'example_data/g102/*raw.fits'\n",
    "dst = 'g102/'\n",
    "for f in glob.glob(src):\n",
    "    shutil.copy(f, dst)\n",
    "\n",
    "# src = '/path/to/your/direct/images/*raw.fits'\n",
    "src = 'example_data/f098m/*raw.fits'\n",
    "dst = 'f098m/'\n",
    "for f in glob.glob(src):\n",
    "    shutil.copy(f, dst)\n",
    "\n",
    "# src = '/path/to/your/direct/images/*raw.fits'\n",
    "src = 'example_data/f105w/*raw.fits'\n",
    "dst = 'f105w/'\n",
    "for f in glob.glob(src):\n",
    "    shutil.copy(f, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d45224",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3.1 Run WFC3 Backsub and Calibrate Data <a id=\"cal\"></a>\n",
    "\n",
    "If you are working with WFC3/IR grism data (G102 and/or G141), we highly advise that you use the [WFC3 Backsub](https://stsci.app.box.com/folder/198794823506) program to process the RAW files into calibrated FLT files (see the [Introduction](intro) for download instructions). The G102 and G141 background sky signal is both variable and made up of multiple components, and the current version of the WFC3 calibration pipeline, `calwf3`, does not have the capability to model and remove this dispersed 2D background.  WFC3 Backsub is designed specifically to assess the level of each of the three components (zodiacal, 1.083 μm HeI emission, and scattered) and remove the signal during calibration. The `back_sub.py` program program still relies on and uses `calwf3` for calibration (e.g. bias correction and dark subtraction), but it employs custom reference files (not available in the Calibration Reference Data System (CRDS)) to measure and remove the multiple sky components before the final \"up-the-ramp\" fitting occurs in `calwf3`.   \n",
    "\n",
    "\n",
    "WFC3 Backsub was originally written by [Dr. Norbert Pirzkal](https://www.stsci.edu/stsci-research/research-directory/nor-pirzkal) (at STScI) for his scientific work with the [Faint Infrared Grism Survey](https://ui.adsabs.harvard.edu/abs/2017ApJ...846...84P/abstract). While the code was originally posted on Dr. Pirzkal's personal GitHub repository, we have taken it and updated some of the syntax and procedures to work with the HSTaXe cookbook environment and have it hosted on [STScI's Box](https://stsci.app.box.com/folder/193831769414) now. A description of the three background components and the methods used in WFC3 Backsub can be found in [WFC3 ISR 2020-04](https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/wfc3/documentation/instrument-science-reports-isrs/_documents/2020/WFC3_IR_2020-04.pdf) (Pirzkal & Ryan). For more information on WFC3 IR calibration as well as the IR variable background please see Chapters [3](https://hst-docs.stsci.edu/wfc3dhb/chapter-3-wfc3-data-calibration/3-3-ir-data-calibration-steps) & [7](https://hst-docs.stsci.edu/wfc3dhb/chapter-7-wfc3-ir-sources-of-error/7-10-time-variable-background) of the [WFC3 Data Handbook](https://hst-docs.stsci.edu/wfc3dhb).\n",
    "\n",
    "The cells below assume you have downloaded the `WFC3_backsub` directory from the provided Box link and have it in the same parent directory as this notebook.<br>\n",
    "First, we will copy the `back_sub.py` file and the custom reference files over to the `grism_ims` directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c50a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src = '/path/to/your/WFC3_backsub/*.py'\n",
    "src = 'WFC3_backsub/back_sub.py'\n",
    "dst = 'g102/'\n",
    "cl1 = os.system(f\"cp {src} {dst}\")\n",
    "\n",
    "# src = '/path/to/your/WFC3_backsub/backsub_data/'\n",
    "src = 'WFC3_backsub/backsub_data/'\n",
    "dst = 'g102/backsub_data/'\n",
    "cl2 = os.system(f\"cp -R {src} {dst}\")\n",
    "\n",
    "if cl1 != 0 or cl2 !=0:\n",
    "    print(\"backsub code and/or reference file data did not copy correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed75388-1e77-47fe-8640-e01de7e165ce",
   "metadata": {},
   "source": [
    "With the `back_sub.py` program, custom reference files, and raw grism FITS files all in the same directory (`grism_ims`), we can call the code with the few command line arguments that it has: `grism`, `ipppss`, and `grey_flat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc97bac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('grism_ims')\n",
    "\n",
    "# set arguments for command line call\n",
    "grism = 'G102'\n",
    "ipppss = 'All'\n",
    "grey_flat = True\n",
    "\n",
    "# create command line call and run backsub\n",
    "cl_input = f\"python back_sub.py '*_raw.fits'  --grism={grism} --ipppss={ipppss} --grey_flat={grey_flat}\"\n",
    "cl = os.system(cl_input)\n",
    "if cl != 0:\n",
    "    print(\"Backsub program did not execute properly.\")\n",
    "    print(\"Be sure `back_sub.py`, `backsub_data` and raw grism files are all in the same directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc0e17-7ad3-4315-93a4-21dc1ca4e193",
   "metadata": {},
   "source": [
    "You should now have calibrated FLT grism files inside of the `grism_ims` directory. With the grism images properly background subtracted and calibrated we can move on to calibrating the direct image exposures. For this, all we need to do is call `calwf3` on the RAW files after passing them through [CRDS `bestref`](https://hst-crds.stsci.edu/static/users_guide/basic_use.html).\n",
    "\n",
    "Before running CRDS `bestref`, we need to set [CRDS environment variables](https://hst-crds.stsci.edu/docs/cmdline_bestrefs/).  We will point to a subdirectory called `crds_cache/` using the `IREF` environment variable. The `IREF` variable is used for WFC3 reference files and different instruments use other variables, e.g., `JREF` for ACS. You have the option to permanently add these environment variables to your user profile by adding the path in your shell's configuration file. If you are using bash, you would edit the `~/.bash_profile` file with lines such as:\n",
    "```\n",
    "export CRDS_PATH=\"$HOME/crds_cache\"\n",
    "export CRDS_SERVER_URL=\"https://hst-crds.stsci.edu\"\n",
    "export iref=\"${CRDS_PATH}/references/hst/iref/\"\n",
    "```\n",
    "If you have already set up the CRDS environment variables you may skip running the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CRDS_SERVER_URL'] = 'https://hst-crds.stsci.edu'\n",
    "os.environ['CRDS_SERVER'] = 'https://hst-crds.stsci.edu'\n",
    "if 'CRDS_PATH' not in os.environ.keys():\n",
    "    os.environ['CRDS_PATH'] = os.path.join(os.environ['HOME'],'crds_cache')\n",
    "if 'iref' not in os.environ.keys():\n",
    "    os.environ['iref'] = '$HOME/crds_cache/references/hst/iref/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d07e56d-9c60-42eb-9a0d-d2c4fe5c23d1",
   "metadata": {},
   "source": [
    "Now that the CRDS environment variables are properly set, we can run the direct RAW files through `bestref` and then `calwf3`. <br>\n",
    "If you have never used `bestrefs` before, the next cell may take a few minutes to download necessary reference file mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b6f0a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir('../f098m')\n",
    "\n",
    "raws = glob.glob('*raw.fits')\n",
    "for file in raws:\n",
    "    cl = f\"crds bestrefs --files {file} --sync-references=1 --update-bestrefs\"\n",
    "    os.system(cl)\n",
    "\n",
    "[calwf3(file) for file in raws]\n",
    "\n",
    "os.chdir('../f105w')\n",
    "\n",
    "raws = glob.glob('*raw.fits')\n",
    "for file in raws:\n",
    "    cl = f\"crds bestrefs --files {file} --sync-references=1 --update-bestrefs\"\n",
    "    os.system(cl)\n",
    "\n",
    "[calwf3(file) for file in raws]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939305b",
   "metadata": {},
   "source": [
    "## 3.2 Verify WCS Information<a id=\"wcs\"></a>\n",
    "\n",
    "It is possible that the WCS in the direct and grism images differ. In this section we will use a function to process all the direct and grism images to verify that the WCS information is consistent throughout. If there is any disagreement in WCS information we call `updatewcs` with the database keyword set to False, which will roll back all the solutions to the original distortion-corrected WCS. For more information regarding HST WCS and improved absolute astrometry please see [WFC3 Instrument Science Report 2022-06](https://ui.adsabs.harvard.edu/abs/2022wfc..rept....6M/abstract) (Mack et al.). For documentations on `updatewcs` please see [here](https://stwcs.readthedocs.io/en/latest/updatewcs.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2baebde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_wcs(images):\n",
    "    \"\"\" A helper function to verify the active world coordinate solutions match and roll them back if they do not \n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    images : list \n",
    "        a list of grism and direct images \n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    N/A\n",
    "    \"\"\"\n",
    "    \n",
    "    direct_wcs = []\n",
    "    grism_wcs = []\n",
    "\n",
    "    for f in images:\n",
    "        # get filter for image to distinguish between direct and grism\n",
    "        filt = fits.getval(f, 'FILTER')\n",
    "        \n",
    "        hdul = fits.open(f)\n",
    "        db_bool = 'WCSCORR' not in hdul\n",
    "        hdul.close()\n",
    "        \n",
    "        try:\n",
    "            # get the active solution from the file's \"SCI\" extension\n",
    "            wcsname = fits.getval(f, 'WCSNAME', ext=('SCI', 1))\n",
    "            if db_bool == True:\n",
    "                updatewcs.updatewcs(f,use_db=db_bool)\n",
    "        except KeyError:\n",
    "            updatewcs.updatewcs(f,use_db=db_bool)\n",
    "            wcsname = fits.getval(f, 'WCSNAME', ext=('SCI', 1))\n",
    "        \n",
    "        # seperate between direct and grism\n",
    "        if 'G' in filt:\n",
    "            grism_wcs.append(wcsname)\n",
    "        if 'F' in filt:\n",
    "            direct_wcs.append(wcsname)\n",
    "\n",
    "    # get the number of unique active solutions in the direct and grism images       \n",
    "    num_wcs_direct = len(set(direct_wcs))\n",
    "    num_wcs_grism = len(set(grism_wcs))\n",
    "\n",
    "    # roll back WCS on all files if there is more than one active solution for either direct or grism images\n",
    "    if num_wcs_direct > 1 or num_wcs_grism > 1:\n",
    "        [updatewcs.updatewcs(file,use_db=False) for file in images]\n",
    "        print('WCS reset complete')\n",
    "\n",
    "    # roll back WCS on all files if the active solution for the direct images do not match the grism images\n",
    "    elif set(direct_wcs) != set(grism_wcs):\n",
    "        [updatewcs.updatewcs(file,use_db=False) for file in images]\n",
    "        print('WCS reset complete')\n",
    "\n",
    "    # do nothing if there is one unique active solution and they match\n",
    "    elif set(direct_wcs) == set(grism_wcs):\n",
    "        print(f\"No WCS update needed. All grism and direct images use WCS: {grism_wcs[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b839a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(cwd)\n",
    "all_images = glob.glob('f098m/i*_flt.fits')+\\\n",
    "             glob.glob('f105w/i*_flt.fits')+\\\n",
    "             glob.glob('g102/i*_flt.fits')\n",
    "check_wcs(all_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f079201a",
   "metadata": {},
   "source": [
    "## 3.3 Drizzling the Input Data<a id=\"drizzle\"></a>\n",
    "The next step is to drizzle the grism images. We'll need a list of the image names to feed to AstroDrizzle. After that, we'll do the same for the direct images, but use the drizzled grism image as a reference, which will ensure proper registration between the data. HSTaXe will use these linked drizzle images to locate spectral traces based on the positions of sources in the direct images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cdf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list file using images in grism directory\n",
    "os.chdir('g102')\n",
    "\n",
    "lis = open('g102.lis', 'w')\n",
    "for f in sorted(os.listdir('.')):\n",
    "    if f[-8:] == 'flt.fits':\n",
    "    if os.path.splitext(f)[1]=='.fits':\n",
    "        lis.write(f)\n",
    "        lis.write('\\n')\n",
    "lis.close()\n",
    "\n",
    "!cat g102.lis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806c612",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drizzle grism images. If only using one input image, set blot, median, driz_cr to False\n",
    "astrodrizzle.AstroDrizzle('@g102.lis', output='g102', mdriztab=True, \n",
    "                          preserve=False, skysub=False, final_fillval=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679312fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List files for direct images\n",
    "os.chdir(cwd)\n",
    "os.chdir('f098m')\n",
    "\n",
    "lis = open('f098m.lis', 'w')\n",
    "for f in sorted(os.listdir('.')):\n",
    "    if f[-8:] == 'flt.fits':\n",
    "    # if os.path.splitext(f)[1]=='.fits':\n",
    "        lis.write(f)\n",
    "        lis.write('\\n')\n",
    "lis.close()\n",
    "\n",
    "os.chdir('../f105w')\n",
    "\n",
    "lis = open('f105w.lis', 'w')\n",
    "for f in sorted(os.listdir('.')):\n",
    "    if f[-8:] == 'flt.fits':\n",
    "    # if os.path.splitext(f)[1]=='.fits':\n",
    "        lis.write(f)\n",
    "        lis.write('\\n')\n",
    "lis.close()\n",
    "\n",
    "!cat f098m.lis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02fdba9b-5801-4adc-befe-2570a013a87b",
   "metadata": {},
   "source": [
    "Next, drizzle the direct images using the drizzled grism mosaic as a reference to ensure proper registration. For more information please see the `AstroDrizzle` documentation [here](https://drizzlepac.readthedocs.io/en/latest/astrodrizzle.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9d4e7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref = '../grism_ims/grism_drz.fits[1]'\n",
    "astrodrizzle.AstroDrizzle(\"@f098m.lis\", output=\"f098m\", mdriztab=True, \n",
    "                          preserve=False, skysub=False, final_fillval=None)\n",
    "astrodrizzle.AstroDrizzle(\"@f105w.lis\", output=\"f105w\", mdriztab=True, \n",
    "                          preserve=False, skysub=False, final_fillval=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9187eb",
   "metadata": {},
   "source": [
    "Your grism and direct images should now be aligned. For the WFC3/IR grisms, there should be very little vertical offset between the positions of sources in the direct image and their correspondents in the grism image. We'll perform a quick visual check here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28fa6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.chdir(cwd)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15,10), dpi=100)\n",
    "\n",
    "d = fits.getdata('direct_ims/direct_drz.fits', 1)\n",
    "z1,z2 = zscale.zscale(d)\n",
    "im1 = axs[0].imshow(d, origin='lower', cmap='Greys_r', vmin=z1, vmax=z2)\n",
    "axs[0].set_title('direct_drz.fits')\n",
    "fig.colorbar(im1,shrink=0.7,pad=0.01)\n",
    "\n",
    "d = fits.getdata('grism_ims/grism_drz.fits', 1)\n",
    "z1,z2 = zscale.zscale(d)\n",
    "im2 = axs[1].imshow(d, origin='lower', cmap='Greys_r', vmin=z1, vmax=z2)\n",
    "axs[1].set_title('grism_drz.fits')\n",
    "fig.colorbar(im2,shrink=0.7,pad=0.01)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30810db8",
   "metadata": {},
   "source": [
    "## 3.4 Creating a Catalog with SExtractor <a id=\"catalog\"></a>\n",
    "\n",
    "This section is intended for anyone using data other than the `example_data` provided for this notebook. Since we also provide a catalog in the `example_data` directory and will not formally run SExtractor here, we want to explain the process behind using the drizzle image to create a new catalog with SExtractor. **Please refer to the links in the [Introduction](#intro) section for instructions regarding installing SExtractor and downloading the necessary aXe-SExtractor configuration files.**\n",
    "\n",
    "HSTaXe will look for a highly specific format in the catalog, and does not always give clear error messages when something within the catalog is awry. If creating a catalog yourself, please follow the next steps carefully:\n",
    "\n",
    "1. Copy the drizzled direct image into the `sextractor` directory (once created), which contains HSTaXe-appropriate configuration files for SExtractor.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<u>Example code:</u>\n",
    "```python\n",
    "shutil.copy('direct_ims/direct_drz.fits', 'sextractor/')\n",
    "```\n",
    "2. With SExtractor installed, run the follow\"ing command from within the `sextractor` directory that you created:\n",
    "   \n",
    "   `sex -c aXe.sex direct_drz.fits[1] -DETECT_THRESH 5 -MAG_ZEROPOINT 26.4525`\n",
    "\n",
    "    Note that the value for the `DETECT_THRESH` keyword, which sets the minimum value for pixels to be considered, may be changed appropriately for your data. `MAG_ZEROPOINT` should also be changed to match the magnitude zeropoint of the direct image filter for your data. A table is provided below containing the pivot wavelengths and zeropoints for the WFC3/IR grism reference filters.\n",
    "    \n",
    "| Filter | Pivot Wavelength (nm) | Zeropoint (ABmag |\n",
    "|:--------|-----------------------|------------------|\n",
    "| F098M  |         986.4         |      25.666      |\n",
    "| F105W  |          1055         |      26.264      |\n",
    "| F140W  |          1392         |      26.450      |\n",
    "| F160W  |          1537         |      25.936      |\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<u>Example code:</u>\n",
    "```python\n",
    "os.chdir('sextractor')\n",
    "detect_thresh = 10\n",
    "cl_input = f'sex -c aXe.sex direct_drz.fits[1] -DETECT_THRESH {detect_thresh} -MAG_ZEROPOINT 25.666'\n",
    "os.system(cl_input)\n",
    "``` \n",
    "3. At this point you should have created a catalog. The next steps include copying the file into the `direct_ims` directory and editing the name of the `MAG_ISO` column. See [Section 2.3](#copycat) for information regarding renaming the `MAG_ISO` column. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<u>Example code:</u>\n",
    "```python\n",
    "os.chdir(cwd)\n",
    "shutil.copy('sextractor/aXe.cat', 'direct_ims')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9f377a",
   "metadata": {},
   "source": [
    "## 3.5 Copy Catalog and Rename Mag Column <a id=\"copycat\"></a>\n",
    "\n",
    "The catalog corresponding to the data used in this notebook is included in the `example_data` directory downloaded in the Introduction section. Start by copying the catalog into the `direct_ims` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087eb4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the example catalog to the direct image directory:\n",
    "os.chdir(cwd)\n",
    "shutil.copy('example_data/aXe.cat', 'direct_ims');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d555574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = Table.read('direct_ims/aXe.cat', format='ascii.sextractor')\n",
    "cols_to_show = ['NUMBER', 'X_IMAGE', 'Y_IMAGE', 'A_IMAGE', 'B_IMAGE', 'MAG_ISO', 'MAGERR_ISO']\n",
    "cat[cols_to_show].show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058a1c3",
   "metadata": {},
   "source": [
    "Examine the catalog. The \"MAG_ISO\" column must be renamed to \"MAG_F####\" for the catalog to be correctly read in by HSTaXe. Where \"####\" is the pivot wavelength of the direct image filter in nm (Å for WFC3/UVIS, e.g. 4971 for F200LP and 1392 for F140W). Please see WFC3 Instrument Handbook Section 7.5 [\"IR Spectral Elements\"](https://hst-docs.stsci.edu/wfc3ihb/chapter-7-ir-imaging-with-wfc3/7-5-ir-spectral-elements) for information about UVIS filter pivot wavelengths. You can also refer to the table above as a quick-reference for the pivot wavelength of the recommended direct image filters for the IR grisms.\n",
    "\n",
    "Any lines in the catalog containing clearly spurious detections, such as those with magnitudes of 99.0, should also be removed. **Note**: Removing spurious detections is not apart of this notebook and will need to be done manually. \n",
    "\n",
    "\n",
    "Lastly, locate the lines containing the sources whose spectra you want to extract and note the line number from the NUMBER column. This will be used later to identify the BEAM number for your object in the output files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee9d2a",
   "metadata": {},
   "source": [
    "To avoid having to edit the column information manually in the SExtractor catalog, there is a helper function below called `edit_catalog_pivot`. It takes in the SExtractor catalog, output file path/name, and the pivot wavelength value and edits the information and writes to the output file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3afd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_catalog_pivot(inputfile, outputfile, pivot_wave):\n",
    "    \"\"\" Function to edit the auto-generated sextractor header/column name so aXe will run\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inputfile : str\n",
    "            The full path to the input catalog including filename\n",
    "        outputfile : str\n",
    "            The full path to the output catalog including filename\n",
    "        pivot_wave : int or str\n",
    "            The pivot wavelength of filter used in the driect image\n",
    "            For UVIS please use 4 digits in units of Angstrom and \n",
    "            for IR please use 4 ditits in units of nanometers \n",
    "            \n",
    "        Return\n",
    "        ------\n",
    "        Nothing. But a file is written to `outputfile`\n",
    "    \"\"\"\n",
    "    # Read in the input catalog\n",
    "    with open(inputfile, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    with open(outputfile, 'w') as f:\n",
    "        # Find the mag_iso row and replace with pivot wavelength\n",
    "        for line in lines:\n",
    "            line = line.replace('MAG_ISO', f'MAG_F{pivot_wave}')\n",
    "            f.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde19974",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputfile = 'direct_ims/aXe.cat'\n",
    "outputfile = 'direct_ims/aXe_ir_f098m.cat'\n",
    "pivot_wave = 986 # nm\n",
    "edit_catalog_pivot(inputfile, outputfile, pivot_wave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ac667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to filter your catalog for your sources\n",
    "# In this example, we sort to identify the brightest sources\n",
    "cat = Table.read(outputfile, format='ascii.sextractor')\n",
    "cat.sort(f'MAG_F{pivot_wave}')\n",
    "cols_to_show = ['NUMBER', 'X_IMAGE', 'Y_IMAGE', 'A_IMAGE', 'B_IMAGE', 'THETA_IMAGE', 'FLUX_RADIUS', 'MAG_F986', 'MAGERR_ISO']\n",
    "cat[cols_to_show].show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b64214",
   "metadata": {},
   "source": [
    "# 4. Running HSTaXe<a id=\"aXe\"></a>\n",
    "\n",
    "With the catalog generated and edited, we can now move on to working with HSTaXe. We'll set up a few additional directories and environment variables that point to them, while clearing out any previous data or outputs in these directories. We'll also copy our data into the fresh `DATA` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef990243",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cwd)\n",
    "\n",
    "for dirr in ['DATA','CONF','OUTPUT', 'DRIZZLE']:\n",
    "    if os.path.isdir(dirr):\n",
    "        shutil.rmtree(dirr)\n",
    "    os.mkdir(dirr)\n",
    "    \n",
    "os.environ['AXE_IMAGE_PATH'] = './DATA/' \n",
    "os.environ['AXE_CONFIG_PATH'] = './CONF/'\n",
    "os.environ['AXE_OUTPUT_PATH'] = './OUTPUT/'\n",
    "os.environ['AXE_DRIZZLE_PATH'] = './DRIZZLE/'\n",
    "\n",
    "dsrc = 'direct_ims/*flt.fits'\n",
    "gsrc = 'grism_ims/*flt.fits'\n",
    "csrc = 'WFC3_IR_conf/*'\n",
    "\n",
    "for files in [dsrc, gsrc, csrc]:\n",
    "    if '_ims' in files:\n",
    "        dirr = 'DATA'\n",
    "    elif 'conf' in files:\n",
    "        dirr = 'CONF'\n",
    "    for f in glob.glob(files):\n",
    "        shutil.copy(f, dirr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0eb2d7",
   "metadata": {},
   "source": [
    "Next, we define the field-of-view boundaries for the detector. We'll pass this information to the `iolprep` task, which will let it include in the input object lists (IOLs) it generates, objects whose direct image locations fall outside of the chip but whose spectral traces do fall onto the chip.\n",
    "\n",
    "For WFC3/IR, the [left, right, top, bottom] extensions, in pixels, are [183, 85, 50, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55227aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOV = '183,85,50,50'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662a6e1",
   "metadata": {},
   "source": [
    "Now we'll run `iolprep` to generate our IOLs, which are object catalogs for each individual direct image, from the drizzled direct image and its catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9068904",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cwd)\n",
    "os.chdir('direct_ims')\n",
    "\n",
    "axetasks.iolprep(drizzle_image = 'direct_drz.fits',\n",
    "                input_cat = 'aXe_ir_f098m.cat',\n",
    "                dimension_in = FOV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the IOLs to the aXe DATA directory\n",
    "os.chdir(cwd)\n",
    "for f in glob.glob('direct_ims/*_?.cat'):\n",
    "    shutil.copy(f, 'DATA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e616117",
   "metadata": {},
   "source": [
    "The last step before extracting our spectra is to generate a file which contains on each line the names of a grism image, IOL name, and associated direct image. This is best done manually, to ensure that each grism image lines up with the appropriate direct image. For the example data, a file called `example.lis` is provided.\n",
    "\n",
    "With this list, we run `axeprep`, which prepares the individual images for spectral extraction. This step is also responsible for perfoming a global background subtraction, if desired. This is controlled by the `backgr` keyword. \n",
    "\n",
    "Note that the `configs` and `backims` keywords should be matched to the data you have. E.g., if you are working with G102 data with direct images in F098M, the config file should be `G102.F098M.V4.32.conf`. If you are not performing a global background subtraction, `backims` is not required, but should be matched to the correct grism if you are. **Note: we do not currently recommend using HSTaXe to perform global background subtraction. A more reliable method will be added to this notebook in the near future.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d9d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the example list file\n",
    "os.chdir(cwd)\n",
    "shutil.copy('example_data/example.lis', '.')\n",
    "os.rename('example.lis', 'aXe.lis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a044ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cwd)\n",
    "\n",
    "axetasks.axeprep(inlist='aXe.lis',\n",
    "                     configs='G102.F098M.V4.32.conf',\n",
    "                     backgr=False,\n",
    "                     norm=False,\n",
    "                     mfwhm=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06bc7f",
   "metadata": {},
   "source": [
    "The last HSTaXe task to run is `axecore`, which performs the actual extraction and generates output files. Again, the configuration file and sky background arguments should be matched to the spectral elements used for your data.\n",
    "\n",
    "Local background subtraction is also performed by this step, if desired. The following keywords are critical for local background:\n",
    "\n",
    "* `back`: This argument is the flag to trigger local background subtraction.\n",
    "* `np`: Defines the number of pixels on either side of the spectral trace (beam) used to calculate the local background from.\n",
    "* `interp`: Sets the interpolation method for the local background (-1=median, 0=mean, ≥1=nth order polynomial)\n",
    "* `backfwhm`: The FWHM specifying the width of the background pixel extraction table\n",
    "\n",
    "More information on background handling, both global and local, with `HSTaXe` can be found in the documentation [here](https://hstaxe.readthedocs.io/en/latest/hstaxe/description.html#sky-background).\n",
    "\n",
    "In addition to local background subtraction, HSTaXe is also able to perform a vertical extraction. This method of extraction requires editing the `THETA_IMAGE` column to `-90.0` in the object catalog (aXe.cat) and setting two keywords in `axecore`: `orient=True` and `slitless_geom=False`. Vertical extractions have been used in the past to handle the extreme curvature of the orders. For more information on the vertical extraction method please see [WFC3 ISR 2011-18](https://ui.adsabs.harvard.edu/abs/2011wfc..rept...18R/abstract) (Rothberg et al. 2011)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147d692",
   "metadata": {},
   "outputs": [],
   "source": [
    "axetasks.axecore('aXe.lis',\n",
    "                 'G102.F098M.V4.32.conf',\n",
    "                 fconfterm=None,\n",
    "                 extrfwhm=4.,\n",
    "                 drzfwhm=3.,\n",
    "                 orient=False,\n",
    "                 back=False,\n",
    "                 weights=True,\n",
    "                 slitless_geom=True,\n",
    "                 cont_model='gauss',\n",
    "                 sampling='drizzle',\n",
    "                 exclude=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef080fa",
   "metadata": {},
   "source": [
    "## 4.1. Outputs<a id=\"out\"></a>\n",
    "\n",
    "Each grism input file will have several corresponding output files. For each of the G102 and G141 input FLT file, HSTaXe will create the following in the `OUTPUT/` directory:\n",
    "\n",
    "- \\<ipppssoot>_flt_2.cat          : Object catalog for the FLT file [ipppssoot]_flt.fits<br>\n",
    "- \\<ipppssoot>_flt_2.OAF          : Aperture file<br>\n",
    "- \\<ipppssoot>_flt_2.PET.fits     : The Pixel Extraction Table, containing all the unbinned information about each spectrum<br>\n",
    "- \\<ipppssoot>_flt_2.SPC.fits     : 1D extracted spectra<br>\n",
    "- \\<ipppssoot>_flt_2.CONT.fits    : Contamination estimate for eact of the spectra<br>\n",
    "- \\<ipppssoot>_flt_2_opt.SPC.fits : Optimally extracted version of the 1D spectra\n",
    "\n",
    "For now, let's take a look at the STP files, which contain 2D \"stamps\" of the extracted spectral traces; and the SPC files, which contain our 1D extracted spectra.\n",
    "\n",
    "We'll need the line numbers from the original source catalog we generated to identify the BEAM number for the object whose spectrum we want. For the example data, The target is the bright planetary nebula in the middle-left of the drizzled direct image. Its number in the example catalog is 19.\n",
    "\n",
    "First, let's examine the stamps from the STP files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d4e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "beam = '19'\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10,6))\n",
    "\n",
    "for i, f in enumerate(glob.glob('OUTPUT/*STP.fits')):\n",
    "    \n",
    "    with fits.open(f) as hdul:\n",
    "        d = hdul[f'BEAM_{beam}A'].data\n",
    "    z1,z2 = zscale.zscale(d)\n",
    "    im = axes[i].imshow(d, origin='lower', vmin=z1, vmax=z2)\n",
    "    fig.colorbar(im,ax=axes[i],shrink=0.5, pad=0.01, aspect=6)\n",
    "    axes[i].set_title(os.path.basename(f))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa703c",
   "metadata": {},
   "source": [
    "And now, we can finally look at our extracted spectra from the SPC files. For the example data, we've plotted the expected location of several emission lines that should be present in the planetary nebula spectrum (from Table 2 in [Bohlin et al. 2015](https://www.stsci.edu/files/live/sites/www/files/home/hst/instrumentation/wfc3/documentation/instrument-science-reports-isrs/_documents/2015/WFC3-2015-10.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f2a7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(10,6), dpi=120)\n",
    "ax.grid(alpha=0.5)\n",
    "\n",
    "for i, f in enumerate(glob.glob('OUTPUT/*2.SPC.fits')):\n",
    "    with fits.open(f) as hdul:\n",
    "        d = hdul[f'BEAM_{beam}A'].data\n",
    "        h = hdul[0].header\n",
    "    wl = d['LAMBDA']\n",
    "    flux = d['FLUX']\n",
    "    error = d['FERROR']\n",
    "    contam = d['CONTAM']\n",
    "    xrange = (wl>8000) & (wl<11500)\n",
    "        \n",
    "    ax.errorbar(wl[xrange],flux[xrange],error[xrange], label=f'{os.path.basename(f)}')\n",
    "        \n",
    "# ax.set_title(os.path.basename(f))\n",
    "ax.axvline(9070.0, ls=':', c='k') # SIII\n",
    "ax.axvline(9535.1, ls=':', c='k') # SIII+P epsilon\n",
    "ax.axvline(10060.2, ls=':', c='k') # HI P7\n",
    "ax.axvline(10833.5, ls=':', c='k') # HeI\n",
    "\n",
    "\n",
    "ax.set_ylabel(r'Flux ($erg/cm^2/s/\\AA$)')\n",
    "ax.set_title(f\"{h['targname']}\",size=13)\n",
    "ax.set_xlabel(r'Wavelength ($\\AA$)')\n",
    "ax.legend(loc='upper left')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76c48f0d",
   "metadata": {},
   "source": [
    "# 5. Fluxcube Extraction <a id=\"fluxcube\"></a>\n",
    "\n",
    "This more advanced extraction method will produce a final result that contains more accurate contamination estimates, and is weighted. We will achieve this by using the `aXedrizzle` functionality from HSTaXe, which will allow us to combine spectra taken at the same orientation. We will also make use of the segmentation map generated by SExtractor to determine the shape of the objects in our images.\n",
    "\n",
    "The following steps will make use of the example data we used for the basic extraction above, as well as some additional images taken with a different direct imaging filter. We did a good deal of the required pre-processing for these images earlier, so we wouldn't have to repeat the same steps. Now, we'll create a directory to create our fluxcube in, and copy the drizzled direct and grism images we made in [Section 3.3](#drizzle). We'll also copy the set of grism images from the HSTaXe DATA directory, and the segmentation map output by SExtractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d1e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(cwd)\n",
    "if os.path.isdir('flx'):\n",
    "    shutil.rmtree('flx')\n",
    "os.mkdir('flx')\n",
    "dst = 'flx'\n",
    "\n",
    "for d in ['g102', 'f098m', 'f105w']:\n",
    "    f = f'{d}/{d}_drz.fits'\n",
    "    shutil.copy(src, dst);\n",
    "\n",
    "src = 'g102'\n",
    "for f in glob.glob(f'{src}/_flt.fits'):\n",
    "    shutil.copy(f, dst);\n",
    "\n",
    "src = 'example_data'\n",
    "shutil.copy(f'{src}]/seg.fits', 'flx');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "73af7c9c",
   "metadata": {},
   "source": [
    "Next, we'll create a file called `cube.lis` that contains a description of the drz images we're using, as well as the pivot wavelengths (in nm) and ABmag zeropoints for our direct filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92f63be",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('flx')\n",
    "direct_ims = glob.glob('f*drz.fits')\n",
    "\n",
    "lis = open('cube.lis', 'w')\n",
    "\n",
    "for f in direct_ims:\n",
    "    photplam = fits.getval(f, 'PHOTPLAM', ext=0)  # pivot wavelength in Angstroms\n",
    "    photflam = fits.getval(f, 'PHOTFLAM', ext=0)  # inverse sensitivity in Angstroms\n",
    "    ab_zeropoint = -48.60 - 2.5*np.log10(photflam * photplam**2/3e18)  # f_nu = lambda**2 / c * f_lambda (c in Angstroms/s since photplam in Angstroms)\n",
    "    line = f'{f} {photplam/10} {ab_zeropoint}'\n",
    "    lis.write(line)\n",
    "    lis.write('\\n')\n",
    "lis.close()\n",
    "\n",
    "!cat cube.lis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1338f",
   "metadata": {},
   "source": [
    "# 6. Conclusions <a id=\"conclusions\"></a>\n",
    "\n",
    "Thank you for walking through this notebook. You should now be able to perform extractions on WFC3/IR spectral data using HSTaXe.\n",
    "\n",
    "For additional information on the WFC3 grisms, please visit the [grism resources](https://www.stsci.edu/hst/instrumentation/wfc3/documentation/grism-resources) and [grism data analysis](https://www.stsci.edu/hst/instrumentation/wfc3/documentation/grism-resources/grism-data-analysis) webpages.\n",
    "\n",
    "Cookbooks walking through extraction methods for WFC3/UVIS and ACS/WFC are available on the [HSTaXe GitHub](https://github.com/spacetelescope/HSTaXe). For detailed information on HSTaXe, please visit the [documentation webpage](https://hstaxe.readthedocs.io/en/latest/index.html). Lastly, if you have questions regarding this notebook or using WFC3 data with HSTaXe please contact our WFC3 [Help Desk](https://stsci.service-now.com/hst).\n",
    "\n",
    "\n",
    "**Congratulations, you have completed the notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a339b9d",
   "metadata": {},
   "source": [
    "## 7. About this Notebook <a id=\"about\"></a>\n",
    "\n",
    "**Author:** Aidan Pidgeon and Benjamin Kuhn, WFC3 Instrument Team\n",
    "\n",
    "**Special Thanks to:** \n",
    " - Dr. Nor Pirzkal, for creating the original workflow that was adapted into this notebook\n",
    " - Ricky O'Steen and Duy Nguyen, for their fantastic work in updating the HSTaXe module\n",
    " - Debopam Som, for support in testing the HSTaXe workflow\n",
    "\n",
    "**Released:** 2023-01-06 <br>\n",
    "**Last Updated:** 2023-03-14\n",
    "\n",
    "## 8. Citations <a id=\"cite\"></a>\n",
    "\n",
    "If you use `astropy`, `drizzlepac`, `matplotlib` or `numpy` for published research, please cite the\n",
    "authors. Follow this link for more information about citing the libraries:\n",
    "\n",
    "* [Citing `astropy`](https://www.astropy.org/acknowledging.html)\n",
    "* [Citing `drizzlepac`](https://drizzlepac.readthedocs.io/en/latest/LICENSE.html)\n",
    "* [Citing `matplotlib`](https://matplotlib.org/stable/users/project/citing.html)\n",
    "* [Citing `numpy`](https://numpy.org/citing-numpy/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa2be1-76a8-4ab9-9816-6fec439c951c",
   "metadata": {},
   "source": [
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e17d4-cca9-48c5-9754-5a0072bcc7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "61185e4153bd22ba192429d01ef156e5c02e7fcbb7ff0f03ca08685bc8232e35"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
